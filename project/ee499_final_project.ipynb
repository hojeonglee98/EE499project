{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE499 Final Project\n",
    "# Premier League Predictor\n",
    "Hojeong Lee and Nate Chism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from autograd import grad \n",
    "from autograd import hessian\n",
    "import math\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "readDataPath = './readData/'\n",
    "sys.path.append('./')\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import svm \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 114)\n",
      "[[0 'https://www.premierleague.com/match/7186' '10/11' ... 0.78 -4.0 55.6]\n",
      " [1 'https://www.premierleague.com/match/7404' '10/11' ... 0.32 17.0 60.2]\n",
      " [2 'https://www.premierleague.com/match/7255' '10/11' ... 0.38 9.0 66.7]\n",
      " ...\n",
      " [4067 'https://www.premierleague.com/match/59178' '20/21' ... 0.69 4.0\n",
      "  64.1]\n",
      " [4068 'https://www.premierleague.com/match/59182' '20/21' ... 0.42 6.0\n",
      "  54.2]\n",
      " [4069 'https://www.premierleague.com/match/59052' '20/21' ... 0.87 5.0\n",
      "  71.1]]\n"
     ]
    }
   ],
   "source": [
    "season_data = pd.read_csv('./readData/df_full_premierleague.csv')\n",
    "season_csv_to_array = np.array(season_data)\n",
    "# print(range(0, len(house_csv_to_array) -1))\n",
    "print(season_csv_to_array.shape)\n",
    "print(season_csv_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# 2, 4, 5, 6, 8 - 31, 38 - 49, 56  \n",
    "print(season_csv_to_array[5][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4070\n",
      "114\n",
      "[678.  -1.   0.]\n"
     ]
    }
   ],
   "source": [
    "i, j = season_csv_to_array.shape\n",
    "print(i)\n",
    "print(j)\n",
    "match_target_scores = np.zeros((i,3))\n",
    "# match_teams = np.array((i, 3))\n",
    "for it_1 in range(0, i):\n",
    "    # store game unique id\n",
    "    match_target_scores[it_1][0] = season_csv_to_array[it_1][0]\n",
    "    #store game unique id, home team, away team\n",
    "    # match_teams[it_1][0] = season_csv_to_array[it_1][0]\n",
    "    # match_teams[it_1][1] = season_csv_to_array[it_1][4]\n",
    "    # match_teams[it_1][2] = season_csv_to_array[it_1][5]\n",
    "    if season_csv_to_array[it_1][34] > 0:\n",
    "        match_target_scores[it_1][1] = 1\n",
    "    elif season_csv_to_array[it_1][34] < 0:\n",
    "        match_target_scores[it_1][1] = -1\n",
    "    else:\n",
    "        match_target_scores[it_1][1] = 0\n",
    "\n",
    "print(match_target_scores[678])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.   10.   15.    5.    9.    4.  609.   65.6   0.   19.    4.   23.\n",
      " 820.    2.   32.    3.    7.    0.  313.   34.4   0.    9.    4.   21.\n",
      " 527.    2.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    0.    0. ]\n"
     ]
    }
   ],
   "source": [
    "season_data_cleaned = np.zeros((i, 102))\n",
    "\n",
    "# print(season_csv_to_array[5])\n",
    "season_csv_to_array[pd.isnull(season_csv_to_array)] = 0\n",
    "# print(season_csv_to_array[5])\n",
    "# 2, 4, 5, 6, 8 - 31, 38 - 49, 56  \n",
    "for it in range(0, i):\n",
    "    ct = 0\n",
    "    for it2 in range(0, j):\n",
    "        \n",
    "        # if(np.isnan(season_csv_to_array[it][it2])):\n",
    "        #     print(season_csv_to_array[it][it2])\n",
    "\n",
    "        if it2 not in [1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37]:\n",
    "            season_data_cleaned[it][ct] = season_csv_to_array[it][it2]\n",
    "            # print(ct)\n",
    "            ct = ct + 1\n",
    "\n",
    "        elif it2 == 2:\n",
    "            season_number = (int)(season_csv_to_array[it][it2][0] + season_csv_to_array[it][it2][1])\n",
    "            season_data_cleaned[it][ct] = season_number\n",
    "            ct = ct + 1\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# print(season_data_cleaned)\n",
    "print(season_data_cleaned[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it:  4069  ct:  102  it2:  113\n"
     ]
    }
   ],
   "source": [
    "print(\"it: \", it, \" ct: \", ct, \" it2: \", it2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    }
   ],
   "source": [
    "print(season_data_cleaned[4000][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.0"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "season_data_cleaned[123][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_train = np.zeros((3419, 102))\n",
    "season_18 = np.zeros((380, 102))\n",
    "season_19 = np.zeros((380, 102))\n",
    "\n",
    "Y_train = np.zeros((3419,))\n",
    "Y_test1 = np.zeros((380,))\n",
    "Y_test2 = np.zeros((380,))\n",
    "\n",
    "ct1 = 0\n",
    "ct2 = 0\n",
    "ct3 = 0\n",
    "for it in range(0, i):\n",
    "    target_index = season_data_cleaned[it][0]\n",
    "    if season_data_cleaned[it][1] not in [18, 19]:\n",
    "        seasons_train[ct1] = season_data_cleaned[it]\n",
    "        Y_train[ct1] = match_target_scores[it][1]\n",
    "        ct1 = ct1 + 1\n",
    "    elif season_data_cleaned[it][1] == 18:\n",
    "        season_18[ct2] = season_data_cleaned[it]\n",
    "        Y_test1[ct2] = match_target_scores[it][1]\n",
    "        ct2 = ct2 + 1\n",
    "        # print(it)\n",
    "    elif season_data_cleaned[it][1] == 19:\n",
    "        season_19[ct3] = season_data_cleaned[it]\n",
    "        Y_test2[ct3] = match_target_scores[it][1]\n",
    "        ct3 = ct3 + 1\n",
    "        # print(it)\n",
    "    else:\n",
    "        print('THIS SHOULD NEVER HAPPEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3419, 102)\n",
      "(380, 102)\n",
      "(380, 102)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(seasons_train.shape)\n",
    "print(season_18.shape)\n",
    "print(season_19.shape)\n",
    "\n",
    "norm = MinMaxScaler().fit(seasons_10to18)\n",
    "\n",
    "seasons_train_normalized = norm.transform(seasons_train)\n",
    "season_18_normalized = norm.transform(season_18)\n",
    "season_19_normalized = norm.transform(season_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas: \n",
    "    # normalize based on different probabilities\n",
    "        #USE X*w0, X*w1, X*w2 for the 3 probabilities\n",
    "    # train on 3 batches of 800\n",
    "    # create a validation set to use for comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = seasons_train_normalized\n",
    "X_test1 = season_18_normalized\n",
    "X_test2 = season_19_normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.622696694940041\n"
     ]
    }
   ],
   "source": [
    "#model 1: Perceptron\n",
    "\n",
    "perc_clf = Perceptron(alpha= .0001, tol=1e-3, random_state=859)\n",
    "perc_clf.fit(X_train, Y_train)\n",
    "Perceptron()\n",
    "\n",
    "print('Score: ', perc_clf.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perc_clf.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_hist = []\n",
    "for it in range(0, 1000):\n",
    "    perc_clf = Perceptron(alpha= .0001, tol=1e-3, random_state=it)\n",
    "    perc_clf.fit(X_train, Y_train)\n",
    "    Perceptron()\n",
    "    score_hist.append(perc_clf.score(X_train, Y_train))\n",
    "                      \n",
    "print('Max score: ', max(score_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_seed = max(score_hist)\n",
    "max_index_seed = score_hist.index(max_value_seed)\n",
    "print(max_index_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = []\n",
    "val = 0.0001\n",
    "for i in range(0, 10000):\n",
    "    alpha_values.append(val)\n",
    "    val = val + 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_hist = []\n",
    "for it in alpha_values:\n",
    "    perc_clf = Perceptron(alpha= it, tol=1e-3, random_state=max_index_seed)\n",
    "    perc_clf.fit(X_train, Y_train)\n",
    "    Perceptron()\n",
    "    score_hist.append(perc_clf.score(X_train, Y_train))\n",
    "             \n",
    "print('Max score: ', max(score_hist))\n",
    "max_value_alpha = max(score_hist)\n",
    "max_index_alpha = score_hist.index(max_value_alpha)\n",
    "print(max_index_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max score: ', max(score_hist))\n",
    "max_value_alpha = max(score_hist)\n",
    "max_index_alpha = score_hist.index(max_value_alpha)\n",
    "print(max_index_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron\n",
      "Score train:  0.622696694940041\n",
      "Score test 1:  0.6842105263157895\n",
      "Score test 2:  0.6105263157894737\n"
     ]
    }
   ],
   "source": [
    "perc_clf = Perceptron(alpha= .0001, tol=1e-3, random_state=859)\n",
    "perc_clf.fit(X_train, Y_train)\n",
    "Perceptron()\n",
    "\n",
    "print('Perceptron')\n",
    "print('Score train: ', perc_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', perc_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', perc_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (liblinear)\n",
      "Score train:  0.6762211172857561\n",
      "Score test 1:  0.7105263157894737\n",
      "Score test 2:  0.6421052631578947\n"
     ]
    }
   ],
   "source": [
    "log_reg_clf = LogisticRegression(multi_class='ovr', random_state = 0, max_iter=10000, solver = 'liblinear').fit(X_train, Y_train)\n",
    "\n",
    "print('Logistic Regression (liblinear)')\n",
    "print('Score train: ', log_reg_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_hist = []\n",
    "# for state_num in range(0, 1000):\n",
    "#     log_reg_clf = LogisticRegression(multi_class='ovr', random_state = state_num, max_iter=10000, solver = 'liblinear').fit(X_train, Y_train)\n",
    "\n",
    "#     score_hist.append(log_reg_clf.score(X_train, Y_train))\n",
    "\n",
    "# print('Max score: ', max(score_hist))\n",
    "# max_value_seed = max(score_hist)\n",
    "# max_index_seed = score_hist.index(max_value_seed)\n",
    "# print('Best random_state = ', max_index_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score:  0.6762211172857561\n",
      "Best random_state =  3\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "print('Max score: ', max(score_hist))\n",
    "max_value_seed = max(score_hist)\n",
    "max_index_seed = score_hist.index(max_value_seed)\n",
    "print('Best random_state = ', max_index_seed)\n",
    "print(len(score_hist))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Newton)\n",
      "Score train:  0.6735887686458029\n",
      "Score test 1:  0.7131578947368421\n",
      "Score test 2:  0.6421052631578947\n"
     ]
    }
   ],
   "source": [
    "#BEST SO FAR\n",
    "log_reg_clf = LogisticRegression(multi_class='ovr', random_state = 0, max_iter=10000, solver = 'newton-cg').fit(X_train, Y_train)\n",
    "\n",
    "print('Logistic Regression (Newton)')\n",
    "print('Score train: ', log_reg_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  [-1.  0.  1.]\n",
      "(102,)\n",
      "(102,)\n",
      "1:  2.697605849575331\n",
      "2:  -4.9945212883351235\n",
      "3:  4.158158258526025\n",
      "predict probabilities:  [0.17238273 0.24843513 0.57918213]\n"
     ]
    }
   ],
   "source": [
    "print('classes: ', log_reg_clf.classes_)\n",
    "\n",
    "print(X_test1[0].shape)\n",
    "print(log_reg_clf.coef_[0].shape)\n",
    "print('1: ', X_test1[41].T@log_reg_clf.coef_[0])\n",
    "print('2: ', X_test1[41].T@log_reg_clf.coef_[1])\n",
    "print('3: ', X_test1[41].T@log_reg_clf.coef_[2])\n",
    "print('predict probabilities: ', log_reg_clf.predict_proba(X_test1)[41])\n",
    "predictions1 = log_reg_clf.predict(X_test1)\n",
    "predictions2 = log_reg_clf.predict(X_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for season 1:  0.7131578947368421\n",
      "Balanced Accuracy Score for season 1:  0.5922598991961139\n",
      "Accuracy Score for season 2:  0.6421052631578947\n",
      "Balanced Accuracy Score for season 2:  0.6110492577597841\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score for season 1: ', metrics.accuracy_score(predictions1, Y_test1))\n",
    "print('Balanced Accuracy Score for season 1: ', metrics.balanced_accuracy_score(predictions1, Y_test1))\n",
    "print('Accuracy Score for season 2: ', metrics.accuracy_score(predictions2, Y_test2))\n",
    "print('Balanced Accuracy Score for season 2: ', metrics.balanced_accuracy_score(predictions2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "0.09753230111739626\n",
      "0.0960201258919822\n",
      "0.8064475729906216\n",
      "==========\n",
      "0.09753230111739626 , 0.19355242700937847 , 1.0\n",
      "==========\n",
      "0.5018519079614863\n",
      "+1\n"
     ]
    }
   ],
   "source": [
    "prob1 = log_reg_clf.predict_proba(X_test1)\n",
    "temp0 = prob1[0][0]\n",
    "temp1 = prob1[0][1]\n",
    "temp2 = prob1[0][2]\n",
    "print('==========')\n",
    "\n",
    "rand_num = np.random.random()\n",
    "\n",
    "print(temp0)\n",
    "print(temp1)\n",
    "print(temp2)\n",
    "\n",
    "thresh0 = temp0\n",
    "thresh1 = temp0+temp1\n",
    "thresh2 = temp0 + temp1 + temp2\n",
    "print('==========')\n",
    "print(thresh0, ',', thresh1, ',', thresh2)\n",
    "print('==========')\n",
    "if rand_num < thresh0:\n",
    "    print(rand_num)\n",
    "    print('-1')\n",
    "elif thresh0 < rand_num < thresh1:\n",
    "    print(rand_num)\n",
    "    print('0')\n",
    "else:\n",
    "    print(rand_num)\n",
    "    print('+1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1 = log_reg_clf.predict_proba(X_test1)\n",
    "prob2 = log_reg_clf.predict_proba(X_test2)\n",
    "\n",
    "max_pred1 = np.zeros(len(X_test1))\n",
    "random_pred1 = np.zeros(len(X_test1))\n",
    "max_pred2 = np.zeros(len(X_test2))\n",
    "random_pred2 = np.zeros(len(X_test2))\n",
    "\n",
    "for it in range(0, len(X_test1)):\n",
    "    max_pred1[it] = np.max(predictions1[it])\n",
    "    temp0 = prob1[it][0]\n",
    "    temp1 = prob1[it][1]\n",
    "    temp2 = prob1[it][2]\n",
    "    pick = np.random.random()\n",
    "\n",
    "    thresh0 = temp0\n",
    "    thresh1 = temp0+temp1\n",
    "    thresh2 = temp0 + temp1 + temp2\n",
    "    # print('==========')\n",
    "    # print(thresh0, ',', thresh1, ',', thresh2)\n",
    "    # print('==========')\n",
    "    if pick < thresh0:\n",
    "        # print(rand_num)\n",
    "        # print('-1')\n",
    "        random_pred1[it] = -1\n",
    "    elif thresh0 < pick < thresh1:\n",
    "        # print(rand_num)\n",
    "        # print('0')\n",
    "        random_pred1[it] = 0\n",
    "    else:\n",
    "        # print(rand_num)\n",
    "        # print('+1')\n",
    "        random_pred1[it] = 1\n",
    "\n",
    "for it in range(0, len(X_test2)):\n",
    "    max_pred2[it] = np.max(predictions2[it])\n",
    "    temp0 = prob2[it][0]\n",
    "    temp1 = prob2[it][1]\n",
    "    temp2 = prob2[it][2]\n",
    "    pick = np.random.random()\n",
    "\n",
    "    thresh0 = temp0\n",
    "    thresh1 = temp0+temp1\n",
    "    thresh2 = temp0 + temp1 + temp2\n",
    "    # print('==========')\n",
    "    # print(thresh0, ',', thresh1, ',', thresh2)\n",
    "    # print('==========')\n",
    "    if pick < thresh0:\n",
    "        # print(rand_num)\n",
    "        # print('-1')\n",
    "        random_pred2[it] = -1\n",
    "    elif thresh0 < pick < thresh1:\n",
    "        # print(rand_num)\n",
    "        # print('0')\n",
    "        random_pred2[it] = 0\n",
    "    else:\n",
    "        # print(rand_num)\n",
    "        # print('+1')\n",
    "        random_pred2[it] = 1\n",
    "\n",
    "model_pred1 = log_reg_clf.predict(X_test1)\n",
    "model_pred2 = log_reg_clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy Score for season 1:   0.7131578947368421\n",
      "Max Accuracy Score for season 1:     0.7131578947368421\n",
      "Random Accuracy Score for season 1:  0.5421052631578948\n",
      "\n",
      "\n",
      "Model Accuracy Score for season 2:   0.6421052631578947\n",
      "Max Accuracy Score for season 2:     0.6421052631578947\n",
      "Random Accuracy Score for season 2:  0.5026315789473684\n"
     ]
    }
   ],
   "source": [
    "print('Model Accuracy Score for season 1:  ', metrics.accuracy_score(model_pred1, Y_test1))\n",
    "print('Max Accuracy Score for season 1:    ', metrics.accuracy_score(max_pred1, Y_test1))\n",
    "print('Random Accuracy Score for season 1: ', metrics.accuracy_score(random_pred1, Y_test1))\n",
    "print('\\n')\n",
    "print('Model Accuracy Score for season 2:  ', metrics.accuracy_score(model_pred2, Y_test2))\n",
    "print('Max Accuracy Score for season 2:    ', metrics.accuracy_score(max_pred2, Y_test2))\n",
    "print('Random Accuracy Score for season 2: ', metrics.accuracy_score(random_pred2, Y_test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946\n",
      "1482\n",
      "991\n",
      "3419\n",
      "3419\n",
      "==========\n",
      "655\n",
      "1093\n",
      "652\n",
      "2400\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "# even size classes\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "big_ct = 0\n",
    "X_train_trunc = np.zeros((2400, 102))\n",
    "Y_train_trunc = np.zeros(2400)\n",
    "\n",
    "\n",
    "for it in range(len(Y_train)):\n",
    "    index = np.random.randint(0, 3419)\n",
    "    if Y_train[it] == 0:\n",
    "        if count < 800:\n",
    "            X_train_trunc[big_ct] = X_train[index]\n",
    "            Y_train_trunc[big_ct] = Y_train[index]\n",
    "            big_ct = big_ct + 1\n",
    "        count = count + 1\n",
    "    if Y_train[it] == 1:\n",
    "        if count1 < 800:\n",
    "            X_train_trunc[big_ct] = X_train[index]\n",
    "            Y_train_trunc[big_ct] = Y_train[index]\n",
    "            big_ct = big_ct + 1\n",
    "        count1 = count1 + 1\n",
    "    if Y_train[it] == -1:\n",
    "        if count2 < 800:\n",
    "            X_train_trunc[big_ct] = X_train[index]\n",
    "            Y_train_trunc[big_ct] = Y_train[index]\n",
    "            big_ct = big_ct + 1\n",
    "        count2 = count2 + 1\n",
    "\n",
    "print(count)\n",
    "print(count1)\n",
    "print(count2)\n",
    "print(count + count1 + count2)\n",
    "print(len(Y_train))\n",
    "\n",
    "count = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "for it in range(len(Y_train_trunc)):\n",
    "    if Y_train_trunc[it] == 0:\n",
    "        # if count < 800:\n",
    "        #     X_train_trunc[big_ct] = X_train[it]\n",
    "        #     Y_train_trunc[big_ct] = Y_train[it]\n",
    "        #     big_ct = big_ct + 1\n",
    "        count = count + 1\n",
    "    if Y_train_trunc[it] == 1:\n",
    "        # if count1 < 800:\n",
    "        #     X_train_trunc[big_ct] = X_train[it]\n",
    "        #     Y_train_trunc[big_ct] = Y_train[it]\n",
    "        #     big_ct = big_ct + 1\n",
    "        count1 = count1 + 1\n",
    "    if Y_train_trunc[it] == -1:\n",
    "        # if count2 < 800:\n",
    "        #     X_train_trunc[big_ct] = X_train[it]\n",
    "        #     Y_train_trunc[big_ct] = Y_train[it]\n",
    "        #     big_ct = big_ct + 1\n",
    "        count2 = count2 + 1\n",
    "\n",
    "print('==========')\n",
    "print(count)\n",
    "print(count1)\n",
    "print(count2)\n",
    "print(count + count1 + count2)\n",
    "print(len(Y_train_trunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Newton)\n",
      "Score train:  0.7041666666666667\n",
      "Score test 1:  0.718421052631579\n",
      "Score test 2:  0.6289473684210526\n"
     ]
    }
   ],
   "source": [
    "# even size classses\n",
    "log_reg_clf = LogisticRegression(multi_class='ovr', random_state = 0, max_iter=10000, solver = 'newton-cg').fit(X_train_trunc, Y_train_trunc)\n",
    "\n",
    "print('Logistic Regression (Newton)')\n",
    "print('Score train: ', log_reg_clf.score(X_train_trunc, Y_train_trunc))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (Newton)\n",
      "Score train:  0.6735887686458029\n",
      "Score test 1:  0.7131578947368421\n",
      "Score test 2:  0.6421052631578947\n"
     ]
    }
   ],
   "source": [
    "#BEST SO FAR\n",
    "log_reg_clf = LogisticRegression(multi_class='ovr', random_state = 0, max_iter=10000, solver = 'newton-cg').fit(X_train, Y_train)\n",
    "\n",
    "print('Logistic Regression (Newton)')\n",
    "print('Score train: ', log_reg_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = log_reg_clf.predict(X_test1)\n",
    "y_pred2 = log_reg_clf.predict(X_test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "standing18 = {}\n",
    "indexofteams = []\n",
    "for i in range(len(season_csv_to_array)):\n",
    "    season_number = (int)(season_csv_to_array[i][2][0] + season_csv_to_array[i][2][1])\n",
    "    # print(season_number)\n",
    "    if season_number == 18:\n",
    "        indexofteams.append(i)\n",
    "        if (season_csv_to_array[i][4]) in standing18:\n",
    "            standing18[season_csv_to_array[i][4]] = 0\n",
    "            # print(season_csv_to_array[i][4])\n",
    "        else:\n",
    "            standing18[season_csv_to_array[i][4]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Manchester United\n",
      "2\n",
      "Leicester City\n",
      "3\n",
      "Wolverhampton Wanderers\n",
      "4\n",
      "Cardiff City\n",
      "5\n",
      "West Ham United\n",
      "6\n",
      "Liverpool\n",
      "7\n",
      "Everton\n",
      "8\n",
      "Southampton\n",
      "9\n",
      "Brighton and Hove Albion\n",
      "10\n",
      "Arsenal\n",
      "11\n",
      "AFC Bournemouth\n",
      "12\n",
      "Manchester City\n",
      "13\n",
      "Watford\n",
      "14\n",
      "Crystal Palace\n",
      "15\n",
      "Chelsea\n",
      "16\n",
      "Burnley\n",
      "17\n",
      "Tottenham Hotspur\n",
      "18\n",
      "Huddersfield Town\n",
      "19\n",
      "Newcastle United\n",
      "20\n",
      "Fulham\n"
     ]
    }
   ],
   "source": [
    "it = 1\n",
    "for key in standing18:\n",
    "    print(it)\n",
    "    print(key)\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Stoke City'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[515], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y_pred1)):\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m y_pred1[ind] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         standing18[season_csv_to_array[i][\u001b[39m4\u001b[39m]] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[39melif\u001b[39;00m y_pred1[ind] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m         standing18[season_csv_to_array[i][\u001b[39m4\u001b[39m]] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Stoke City'"
     ]
    }
   ],
   "source": [
    "i= 2664\n",
    "for ind in range(len(y_pred1)):\n",
    "    if y_pred1[ind] == 1:\n",
    "        standing18[season_csv_to_array[i][4]] += 3\n",
    "    elif y_pred1[ind] == 0:\n",
    "        standing18[season_csv_to_array[i][4]] += 1\n",
    "        standing18[season_csv_to_array[i][5]] += 1\n",
    "    else:\n",
    "        standing18[season_csv_to_array[i][5]] += 3\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict19 = dict(sorted(standing18.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sorted_dict19)\n",
    "ct = 0\n",
    "print('2019-2020 Premier League Predicted Standings')\n",
    "for it in sorted_dict19:\n",
    "    print(ct, '\\t', it, ':', sorted_dict19[it])\n",
    "    ct = ct+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values_19 = {\n",
    "            'Liverpool' : (1, 99),\n",
    "            'Manchester City' : (2, 81),\n",
    "            'Manchester United' : (3, 66),\n",
    "            'Chelsea' : (4, 66),\n",
    "            'Leicester City' : (5, 62),\n",
    "            'Tottenham Hotspur' : (6, 59),\n",
    "            'Wolverhampton Wanderers' : (7, 59),\n",
    "            'Arsenal' : (8, 56),\n",
    "            'Sheffield United' : (9, 54),\n",
    "            'Burnley' : (10, 54),\n",
    "            'Southampton' : (11, 52),\n",
    "            'Everton' : (12, 49),\n",
    "            'Newcastle United' : (13, 44),\n",
    "            'Crystal Palace' : (14, 43),\n",
    "            'Brighton and Hove Albion' : (15, 41),\n",
    "            'West Ham United' : (16, 39),\n",
    "            'Aston Villa' : (17, 35),\n",
    "            'AFC Bournemouth' : (18, 34),\n",
    "            'Watford' : (19, 34),\n",
    "            'Norwich City' : (20, 21),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_key = max(len(k) for k in sorted_dict19.keys())  # get the maximum length of keys\n",
    "max_len_value = max(len(str(v)) for v in sorted_dict19.values())  # get the maximum length of values\n",
    "place = 1\n",
    "\n",
    "print('2019-2020 Premier League Season')\n",
    "print('# \\t Team \\t\\t\\t\\t Points (pred)\\tPoints (real)\\tPoints Diff\\tPlace Diff')\n",
    "print('__________________________________________________________________________________________________')\n",
    "for key, value in sorted_dict19.items():\n",
    "    if place < 10:\n",
    "        print(place, ' :\\t', \"{:{key_width}}  \\t {:{value_width}} \\t\\t {:{value_width}}\\t\\t {:{value_width}}\\t\\t {:{value_width}}\".format(key, value, actual_values_19[key][1], value - actual_values_19[key][1], actual_values_19[key][0] - place, key_width=max_len_key, value_width=max_len_value))\n",
    "    else:\n",
    "        print(place, ':\\t', \"{:{key_width}}  \\t {:{value_width}} \\t\\t {:{value_width}}\\t\\t {:{value_width}}\\t\\t {:{value_width}}\".format(key, value, actual_values_19[key][1], value - actual_values_19[key][1], actual_values_19[key][0] - place, key_width=max_len_key, value_width=max_len_value))\n",
    "    place = place + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champions_leage_pred = []\n",
    "champions_league_real = ['Liverpool', 'Manchester City', 'Manchester United', 'Chelsea']\n",
    "relegation_pred = []\n",
    "relegation_real = ['AFC Bournemouth', 'Watford', 'Norwich City']\n",
    "\n",
    "place = 1\n",
    "for key, value in sorted_dict19.items():\n",
    "    if place < 5:\n",
    "        champions_leage_pred.append(key)\n",
    "    if place > 17:\n",
    "        relegation_pred.append(key)\n",
    "    place = place + 1\n",
    "print('2019-2020 Premier League Season')\n",
    "print('Champion League Qualification')\n",
    "print('Predicted Teams \\t\\t Actual Teams')\n",
    "print('_________________________________________________')\n",
    "for it in range(0, len(champions_leage_pred)):\n",
    "    print(\"{:{key_width}}\\t{:{key_width}}\".format(champions_leage_pred[it], champions_league_real[it], key_width=max_len_key))\n",
    "print('\\n')\n",
    "print('Relegation Battle')\n",
    "print('Predicted Teams \\t\\t Actual Teams')\n",
    "print('_________________________________________________')\n",
    "for it in range(len(relegation_pred)):\n",
    "    print(\"{:{key_width}}\\t{:{key_width}}\".format(relegation_pred[it], relegation_real[it], key_width=max_len_key))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standing19 = {}\n",
    "indexofteams = []\n",
    "for i in range(len(season_csv_to_array)):\n",
    "    season_number = (int)(season_csv_to_array[i][2][0] + season_csv_to_array[i][2][1])\n",
    "    if season_number == 19:\n",
    "        indexofteams.append(i)\n",
    "        if (season_csv_to_array[i][4]) in standing19:\n",
    "            standing19[season_csv_to_array[i][4]] = 0\n",
    "            # print(season_csv_to_array[i][4])\n",
    "        else:\n",
    "            standing19[season_csv_to_array[i][4]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i= 3799\n",
    "for ind in range(len(y_pred2)):\n",
    "    if y_pred2[ind] == 1:\n",
    "        standing19[season_csv_to_array[i][4]] += 3\n",
    "    elif y_pred2[ind] == 0:\n",
    "        standing19[season_csv_to_array[i][4]] += 1\n",
    "        standing19[season_csv_to_array[i][5]] += 1\n",
    "    else:\n",
    "        standing19[season_csv_to_array[i][5]] += 3\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict20 = dict(sorted(standing19.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sorted_dict20)\n",
    "ct = 0\n",
    "print('2019-2020 Premier League Predicted Standings')\n",
    "for it in sorted_dict20:\n",
    "    print(ct, '\\t', it, ':', sorted_dict20[it])\n",
    "    ct = ct+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values_19 = {\n",
    "            'Liverpool' : (1, 99),\n",
    "            'Manchester City' : (2, 81),\n",
    "            'Manchester United' : (3, 66),\n",
    "            'Chelsea' : (4, 66),\n",
    "            'Leicester City' : (5, 62),\n",
    "            'Tottenham Hotspur' : (6, 59),\n",
    "            'Wolverhampton Wanderers' : (7, 59),\n",
    "            'Arsenal' : (8, 56),\n",
    "            'Sheffield United' : (9, 54),\n",
    "            'Burnley' : (10, 54),\n",
    "            'Southampton' : (11, 52),\n",
    "            'Everton' : (12, 49),\n",
    "            'Newcastle United' : (13, 44),\n",
    "            'Crystal Palace' : (14, 43),\n",
    "            'Brighton and Hove Albion' : (15, 41),\n",
    "            'West Ham United' : (16, 39),\n",
    "            'Aston Villa' : (17, 35),\n",
    "            'AFC Bournemouth' : (18, 34),\n",
    "            'Watford' : (19, 34),\n",
    "            'Norwich City' : (20, 21),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_key = max(len(k) for k in sorted_dict19.keys())  # get the maximum length of keys\n",
    "max_len_value = max(len(str(v)) for v in sorted_dict19.values())  # get the maximum length of values\n",
    "place = 1\n",
    "\n",
    "print('2019-2020 Premier League Season')\n",
    "print('# \\t Team \\t\\t\\t\\t Points (pred)\\tPoints (real)\\tPoints Diff\\tPlace Diff')\n",
    "print('__________________________________________________________________________________________________')\n",
    "for key, value in sorted_dict19.items():\n",
    "    if place < 10:\n",
    "        print(place, ' :\\t', \"{:{key_width}}  \\t {:{value_width}} \\t\\t {:{value_width}}\\t\\t {:{value_width}}\\t\\t {:{value_width}}\".format(key, value, actual_values_19[key][1], value - actual_values_19[key][1], actual_values_19[key][0] - place, key_width=max_len_key, value_width=max_len_value))\n",
    "    else:\n",
    "        print(place, ':\\t', \"{:{key_width}}  \\t {:{value_width}} \\t\\t {:{value_width}}\\t\\t {:{value_width}}\\t\\t {:{value_width}}\".format(key, value, actual_values_19[key][1], value - actual_values_19[key][1], actual_values_19[key][0] - place, key_width=max_len_key, value_width=max_len_value))\n",
    "    place = place + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf = LogisticRegressionCV(multi_class='ovr', random_state = 13, max_iter=1000, solver = 'liblinear').fit(X_train, Y_train)\n",
    "\n",
    "print('Logistic RegressionCV (newton-cholesky)')\n",
    "print('Score train: ', log_reg_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf = RidgeClassifier(alpha=0.001, random_state = 13, max_iter=1000, solver = 'lbfgs', positive=True).fit(X_train, Y_train)\n",
    "\n",
    "print('Ridge Classifier (auto)')\n",
    "print('Score train: ', log_reg_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', log_reg_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', log_reg_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_hist = []\n",
    "# nonlinear SVM\n",
    "for seed in range(1, 100):\n",
    "    svm_clf = svm.SVC(gamma='scale', degree=3, random_state=seed)\n",
    "    svm_clf.fit(X_train, Y_train)\n",
    "\n",
    "    score_hist.append(svm_clf.score(X_train, Y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(gamma='scale', degree=2, random_state=0, kernel='poly')\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "\n",
    "print('SVM SVC')\n",
    "# print('degree = ', deg)\n",
    "print('Score train: ', svm_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', svm_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', svm_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.LinearSVC(penalty= 'l2', loss= 'hinge',random_state=seed)\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "\n",
    "score_hist.append(svm_clf.score(X_train, Y_train))\n",
    "\n",
    "print('SVM LinearSVC')\n",
    "print('Score train: ', svm_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', svm_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', svm_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "weights = {-1:1.0, 0:1.0, -1:1.0}\n",
    "\n",
    "train_results_hist_depth = []\n",
    "test_results_hist_depth1 = []\n",
    "test_results_hist_depth2 = []\n",
    "\n",
    "for deep in range(1,20):\n",
    "    rfc = RandomForestClassifier(random_state = 42, max_depth=18, criterion='gini', class_weight=weights, n_estimators=deep)\n",
    "    rfc.fit(X_train, Y_train)\n",
    "    y_pred_train = rfc.predict(X_train) \n",
    "    y_pred_test1 = rfc.predict(X_test1)  \n",
    "    y_pred_test2 = rfc.predict(X_test2)\n",
    "\n",
    "    train_results_hist_depth.append(metrics.balanced_accuracy_score(Y_train, y_pred_train))\n",
    "    test_results_hist_depth1.append(metrics.balanced_accuracy_score(Y_test1, y_pred_test1))\n",
    "    test_results_hist_depth2.append(metrics.balanced_accuracy_score(Y_test2, y_pred_test2)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,20), train_results_hist_depth, 'xb-', label='train')\n",
    "plt.plot(range(1,20), test_results_hist_depth1, 'sy-', label='test1')\n",
    "plt.plot(range(1,20), test_results_hist_depth2, 'sy-', label='test2')\n",
    "plt.title('Accuracy vs Min Samples Leaf')\n",
    "plt.xlabel('Min Samples Split')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boosting!\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gd_boost_clf = GradientBoostingClassifier(n_estimators=7, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, Y_train)\n",
    "\n",
    "print('GradientBoostingClassifier')\n",
    "print('Score train: ', gd_boost_clf.score(X_train, Y_train))\n",
    "print('Score test 1: ', gd_boost_clf.score(X_test1, Y_test1))\n",
    "print('Score test 2: ', gd_boost_clf.score(X_test2, Y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee499_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
